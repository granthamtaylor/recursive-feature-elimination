{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serverless Workflow Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During my nearly three years at Capital One, I was a strong proponent of Workflow Orchestration and championed [Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/v2/) (KFP). I aided many teams in exploring workflow orchestration, consulted with nearly a hundred individuals, and pioneered best practices.\n",
    "\n",
    "All the same, I found myself feeling quite limited by KFP's strict and non-intuitive implementation. Over the last six months I have reimagined my entire ML engineering philosophy around [Flyte](https://docs.flyte.org/en/latest/index.html), a modern, open-source, pythonic workflow orchestrator.\n",
    "\n",
    "However, as an independent AI researcher, I do not have access to the reliable, scalable Kubernetes infrastructure that enables large organizations. I do not have the expertise to manage my own Kubernetes cluster with accelerated hardware.\n",
    "\n",
    "When I first began my independent work, I was able to leverage the many benefits of Flyte while still executing my projects on my local homelab. However, as my work grew in complexity and required greater scalability, I began to outgrow my local machine.\n",
    "\n",
    "A few months ago, [Union](https://www.union.ai), the developers behind Flyte, reached out with the opportunity to experiment with their Union Serverless Offering. Just as it sounds, Union Serverless provides individuals and small organizations access to a managed Kubernetes cluster to execute their workflows. Being \"serverless\", users only pay for the resources they actively use without compromise to scale or complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Orchestration\n",
    "\n",
    "Before discussing my experience with Union Serverless, I would like to establish some basic terminology and background on the state of workflow orchestration.\n",
    "\n",
    "Workflow orchestration is a ML Engineering / MLOps framework to structure data science projects.\n",
    "\n",
    "Workflow orchestration attempts to isolate the individual steps required to accomplish a larger body of work into a _\"task\"_. Tasks are authored with high-level code, and may have any number of inputs and outputs. Each task may have its own unique runtime environment defined by, for example, a specified version of Python and a list of python libraries. An environment is also defined by its hardware requirements, such as compute (some amount of memory and CPU cores) as well as accelerated hardware (some number of GPUs).\n",
    "\n",
    "The ability for tasks to be authored in heterogeneous environments is a significant advantage of workflow orchestration. Suppose you have some model training workflow that requires a data collection phase (executing a Snowflow query), a feature engineering phase (distributed compute with Spark), and a model training phase (PyTorch with a high-end GPU). Without workflow orchestration, these conflicting environment requirements would be unpleasant to manage. Additionally, workflow orchestration allows for parallelized task execution and task caching.\n",
    "\n",
    "Tasks may be _orchestrated_ together to create a _\"workflow\"_. Workflows represent a larger body of work. Workflows are defined by tasks and even other workflows, such that you may embed workflows within workflows.\n",
    "\n",
    "Flyte, being a modern take on workflow orchestation, takes the idea a bit further than other workflow orchestration engines with the following polished features:\n",
    "1. [Dynamic workflows](https://docs.flyte.org/projects/cookbook/en/v0.3.66/auto/core/control_flow/dynamics.html) to support dynamic iteration and recursion.\n",
    "2. [Task parallelism](https://docs.flyte.org/en/latest/user_guide/advanced_composition/map_tasks.html) to run the same task in parallel with varying inputs among multiple instances and collect their outputs in a subsequent task.\n",
    "3. [Agents](https://docs.flyte.org/en/latest/flyte_agents/index.html) to manage of asynchronous operations, such as long-running queries or a model deployment.\n",
    "4. [Artifact](https://docs.flyte.org/en/latest/api/flytekit/generated/flytekit.Artifact.html) management and querying.\n",
    "\n",
    "This is just scratching the surface of Flyte's capabilities as an orchestration engine, however their [strong documentation](https://docs.flyte.org/en/latest/index.html) does a better job of highlighting the library's sophistication than I possibly could."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common pattern in traditional machine learning is the process of iteratively pruning tabular features from a model in order to reduce its complexity. An ideal traditional machine learning model should be as simple as possible while still achieving its desired outcomes. By removing unnecessary features, a model developer is able to potentially improve generalizability, long-term stability, reduce inference costs, and potentially reduce model inference latency.\n",
    "\n",
    "However, iterative feature pruning is hard to optimize effectively. It requires that the model developer train a larger number of models in order to test how much each feature contributes. Instead of training a large number of models sequentially, we can utilize workflow orchestration to parallelize the training of many models.\n",
    "\n",
    "I have defined the following logic to iteratively prune features from a tabular model:\n",
    "1. Train a model with all of the features present to establish a baseline\n",
    "2. For each feature used to train the original model, remove that feature and train a new model\n",
    "3. Identify the feature that, when removed, results in the smallest decrease in model performance as the \"least important feature\"\n",
    "4. Permanently remove the \"least important feature\" from the current baseline to establish a new baseline\n",
    "5. Repeat the previous steps until there is only one feature remaining\n",
    "\n",
    "If one were to execute this algorithm sequentially, it would require the following time complexity equal to the number of features: $\\dfrac{N(N+1)}{2}$, which simplifies to $O(N^2)$\n",
    "\n",
    "However, if we could parallelize the second step (training a model for each remaining feature, having removed that feature), we could effectively simplify the time complexity to $O(N)$.\n",
    "\n",
    "![image](dag.drawio.svg)\n",
    "\n",
    "This operation is particularly complex, however. We may implement it in Flyte with a recursive loop using Flyte's `flytekit.dynamic` decorator. Within each recursion, we utilize task parallelism (`flytekit.map_task`). As the name suggests, dynamic workflows have an undetermined DAG at workflow compilation time and will be constructed during runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from functools import partial\n",
    "\n",
    "import flytekit\n",
    "from mashumaro.mixins.json import DataClassJSONMixin\n",
    "\n",
    "# Flyte likes to use dataclasses to pass around complex objects\n",
    "# Flyte will handle the serialization / deserialization of these objects for you!\n",
    "@dataclasses.dataclass\n",
    "class Result(DataClassJSONMixin):\n",
    "    \n",
    "    \"\"\"Serializable model result\n",
    "    \"\"\"\n",
    "    \n",
    "    name: str # feature that was removed to create the model\n",
    "    value: float # the performance of the model\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class ResultHistory(DataClassJSONMixin):\n",
    "    \n",
    "    \"\"\"Serializable collection of model pruning iteration results\n",
    "    \"\"\"\n",
    "    \n",
    "    results: list[list[Result]]\n",
    "    \n",
    "    @classmethod\n",
    "    def new(cls) -> \"ResultHistory\":\n",
    "        \n",
    "        return cls(results=[])\n",
    "    \n",
    "@flytekit.task\n",
    "def train(\n",
    "    features: list[str],\n",
    "    dataset: flytekit.types.file.FlyteFile,\n",
    "    remove: str\n",
    ") -> Result:\n",
    "    ...\n",
    "    \n",
    "    # read a dataset, select available features\n",
    "    # train a model with `features`, excluding `remove`\n",
    "    # return a `Result` with the name of the removed feature and the model performance\n",
    "    \n",
    "@flytekit.task\n",
    "def prune(results: list[Result]) -> list[str]:\n",
    "    ...\n",
    "    \n",
    "    # find the least important feature\n",
    "    # return all features except for least important feature\n",
    "    \n",
    "@flytekit.task\n",
    "def append(history: ResultHistory, results: list[Result]) -> ResultHistory:\n",
    "    ...\n",
    "    \n",
    "    # append the latest recursion of results to the result history and return it\n",
    "\n",
    "\n",
    "@flytekit.dynamic\n",
    "def recursively_prune_features(\n",
    "    depth: int,\n",
    "    features: list[str],\n",
    "    dataset: flytekit.types.file.FlyteFile,\n",
    ") -> ResultHistory:\n",
    "    \n",
    "    # the requested recursion depth should never be larger than the number of available features\n",
    "    assert depth < len(features)\n",
    "    \n",
    "    # create new result history before starting recursion\n",
    "    history = ResultHistory.new()\n",
    "    \n",
    "    # begin recursing over features, iteratively pruning the least important ones\n",
    "    for _ in range(depth):\n",
    "        \n",
    "        # train multiple models in parallel\n",
    "        trainer = partial(train, features=features, dataset=dataset)\n",
    "        results = flytekit.map_task(trainer)(remove=features)\n",
    "        \n",
    "        # identify least important feature\n",
    "        features = prune(results=results)\n",
    "\n",
    "        # save recursion results\n",
    "        history = append(history=history, results=results)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am blown away by the elegance of Flyte's SDK. It feels so natural and intuitive that I was able to create this basic workflow in my first attempt! Including all utility functions, documentation, and plotting functions, this parallelized, cached, dynamic workflow took only 300 lines of Python to construct. My source code is available here.\n",
    "\n",
    "After removing all but the last remaining feature, we may plot the history to find the relative feature importances for each recursion and the marginal impact to model performance regardless if the model used is a \"black box\". Flyte provides the ability to render static html directly to their web UI. For the following visualization, I render a Plotly figure and save its underlying `html` to a Flyte Deck.\n",
    "\n",
    "![image](history.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the [California Housing dataset](https://www.kaggle.com/datasets/camnugent/california-housing-prices) to train a simple linear regression model without any feature engineering. The plot visualizes the model performance (R²) as a function of removing any one of the remaining features for an iteration until all but one feature has been removed.\n",
    "\n",
    "As the plot above illustrates, the feature `MedInc` (Median Income) is the most important feature because removing it results in the worst performing model among every recursion. Additionally, `Latitude` and `Longitude` and also of great value to the model. The other features are not nearly as important as these three.\n",
    "\n",
    "Among more complex business problems with significantly larger data, training $\\dfrac{100^2}{2}$ model variations does come with a price, but that price is same whether the models are trained in parallel or sequentially. For some modeling problems at scale, limiting the number of features to the absolute minimum while maintaining optimal performance is absolutely paramount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My take on Serverless\n",
    "\n",
    "The problem is fun, and I would argue it makes for a very practical \"hello world\" for workflow orchestration. However, I have seen many practicing model developers struggle to implement such an analysis simply because of the limitations of their infrastructure combined with a larger number of candidate features (100+). However, by using workflow orchestration to manage the execution of your work, implementing this becomes quite trivial.\n",
    "\n",
    "However, Union Serverless allows individuals such as myself, as well as small teams, to utilize advanced infrastructure without having to dig deep into Docker or Kubernetes. While I do personally think it is valuable for data scientists to be familiar with Docker and Kubernetes, FlyteKit effectively abstracts away all of their complexities such that it is not required. I did not have to bother with configuring a Kubernetes cluster, create any account for a cloud provider (AWS, GCP, Azure), or provide any configuration or settings - Union Serverless works right out of the box. I did not have to install Docker or build any images. Union Serverless builds my images for me in the cloud without any hassle. I did not have to import my local libraries / scripts - FlyteKit will automatically handle my local scripts behind the scenes.\n",
    "\n",
    "On the same note, it took me less than three minutes to onboard to Union Serverless. I was able to authenticate using integration with GitHub. There is no \"password\" or SSH token required.\n",
    "\n",
    "As for my model development experience, I effectively trained hundreds of models with the press of a button in arbitrarily complex environments (including GPUs and high memory instances). I am able to conveniently execute pipelines, manage secrets and artifacts with a local command line interface (`unionai`). I am able author FlyteKit tasks from the comfort on my preferred IDE on my local machine. I was also surprised by how quickly my tasks provision. I am not waiting minutes for a task to start (as I had experienced with KFP) - my tasks provision in mere seconds.\n",
    "\n",
    "In general Union Serverless makes for a very compelling offering for small teams of or individual data scientists, analysts, or engineers who want to get stuff done quickly and reliably without having to bother with cloud providers, managing their own cluster, storage, secrets, authentication, and images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workflow-Y99JGElJ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
